{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path, PureWindowsPath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Cornelius\\\\Cory Dropbox\\\\Cory LeRoy\\\\PC\\\\Documents\\\\GitHub\\\\Store-Sales'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Cornelius/Cory Dropbox/Cory LeRoy/PC/Documents/GitHub/Store-Sales')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_cwd = Path(PureWindowsPath(os.path.dirname(os.getcwd())))\n",
    "path_cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Cornelius/Cory Dropbox/Cory LeRoy/PC/Documents/GitHub/Store-Sales/data')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = path_cwd / 'data'\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path / 'train.csv')\n",
    "df_transaction = pd.read_csv(path / 'transactions.csv')\n",
    "df_holidays = pd.read_csv(path / 'holidays_events.csv')\n",
    "df_oil = pd.read_csv(path / 'oil.csv')\n",
    "df_stores = pd.read_csv(path / 'stores.csv')\n",
    "df_test = pd.read_csv(path / 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shorten date ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_store_merge(train, store):\n",
    "    df_train_store_merged = pd.merge(train, store, how='left', on='store_nbr')\n",
    "    return df_train_store_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train clean\n",
    "\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "first_sale_date_per_store = df_train[df_train['sales'] > 0].groupby('store_nbr')['date'].min().reset_index()\n",
    "\n",
    "# remove rows before stores were open. only do this to train\n",
    "df_train_min_date = pd.merge(df_train, first_sale_date_per_store, on='store_nbr')\n",
    "df_train_shortened = df_train_min_date[df_train_min_date['date_x'] >= df_train_min_date['date_y']] \n",
    "df_train_shortened = df_train_shortened.drop(['date_y'], axis=1)\n",
    "df_train_shortened.rename(columns={'date_x':'date'}, inplace=True)\n",
    "df_train_shortened = train_to_store_merge(df_train_shortened, df_stores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dates when stores were temporarily closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### holidays feature eng ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holidays \n",
    "\n",
    "df_holidays['date'] = pd.to_datetime(df_holidays['date'])\n",
    "df_holidays_real = df_holidays[df_holidays['transferred']==False] \n",
    "\n",
    "#unique_holis2 = df_holidays_real['description'].drop_duplicates()\n",
    "unique_holis2 = df_holidays_real[['description','locale']].drop_duplicates()\n",
    "unique_holis_national = unique_holis2[unique_holis2['locale']=='National'].drop(['locale'],axis=1)\n",
    "unique_holis_city = unique_holis2[unique_holis2['locale']=='Local'].drop(['locale'],axis=1)\n",
    "unique_holis_state = unique_holis2[unique_holis2['locale']=='Regional'].drop(['locale'],axis=1)\n",
    "\n",
    "national_holidays = df_holidays_real[df_holidays_real['locale']=='National'].loc[:,('date','description')]\n",
    "local_holidays = df_holidays_real[df_holidays_real['locale']=='Local'].loc[:,('date','description','locale_name')]\n",
    "state_holidays = df_holidays_real[df_holidays_real['locale']=='Regional'].loc[:,('date','description','locale_name')]\n",
    "\n",
    "df_train_summed_daily = df_train_shortened.groupby(['date','city','state']).agg({'onpromotion':'sum', 'sales':'sum'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holi = pd.merge(df_train_summed_daily, national_holidays, how='left', on='date')\n",
    "df_holi = pd.merge(df_holi, state_holidays, how='left', left_on=['date', 'state'], right_on=['date','locale_name'])\n",
    "df_holi = pd.merge(df_holi, local_holidays, how='left', left_on=['date', 'city'], right_on=['date','locale_name'])\n",
    "\n",
    "df_holi = df_holi.drop(['locale_name_x','locale_name_y'],axis=1)\n",
    "df_holi = df_holi.rename(columns = {'description_x':'national_holiday','description_y':'state_holiday','description':'city_holiday'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cornelius\\AppData\\Local\\Temp\\ipykernel_24680\\194147805.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_holi[holiday] = df_holi['city_holiday'] == holiday\n",
      "C:\\Users\\Cornelius\\AppData\\Local\\Temp\\ipykernel_24680\\194147805.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_holi[holiday] = df_holi['city_holiday'] == holiday\n",
      "C:\\Users\\Cornelius\\AppData\\Local\\Temp\\ipykernel_24680\\194147805.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_holi[holiday] = df_holi['city_holiday'] == holiday\n",
      "C:\\Users\\Cornelius\\AppData\\Local\\Temp\\ipykernel_24680\\194147805.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_holi[holiday] = df_holi['city_holiday'] == holiday\n",
      "C:\\Users\\Cornelius\\AppData\\Local\\Temp\\ipykernel_24680\\194147805.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_holi[holiday] = df_holi['city_holiday'] == holiday\n",
      "C:\\Users\\Cornelius\\AppData\\Local\\Temp\\ipykernel_24680\\194147805.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_holi[holiday] = df_holi['city_holiday'] == holiday\n",
      "C:\\Users\\Cornelius\\AppData\\Local\\Temp\\ipykernel_24680\\194147805.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_holi[holiday] = df_holi['city_holiday'] == holiday\n"
     ]
    }
   ],
   "source": [
    "# set boolean column for each unique holiday. still have dups\n",
    "\n",
    "for holiday in unique_holis_national['description'].tolist():\n",
    "    df_holi[holiday] = df_holi['national_holiday'] == holiday\n",
    "for holiday in unique_holis_state['description'].tolist():\n",
    "    df_holi[holiday] = df_holi['state_holiday'] == holiday\n",
    "for holiday in unique_holis_city['description'].tolist():\n",
    "    df_holi[holiday] = df_holi['city_holiday'] == holiday\n",
    "\n",
    "df_holi = df_holi.drop(['national_holiday','state_holiday','city_holiday'] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines duplicates rows into 1 rows where there is a true for each holiday that falls on the given date\n",
    "unique_holis_list = list(unique_holis2['description'])\n",
    "agg_func = {col: 'any' for col in unique_holis_list}\n",
    "\n",
    "aggregated_df = df_holi.groupby(['date', 'sales', 'city', 'state', 'onpromotion']).agg(agg_func).reset_index()\n",
    "\n",
    "\n",
    "aggregated_df = pd.get_dummies(aggregated_df, columns=['city', 'state'], prefix=['city', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS to determine impact of each holiday across all stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonality has big impact on predicted sales so need to add some variables to capture \n",
    "aggregated_df_ols = aggregated_df.copy()\n",
    "aggregated_df_ols['dow'] = aggregated_df_ols['date'].dt.dayofweek\n",
    "aggregated_df_ols['month'] = aggregated_df_ols['date'].dt.month\n",
    "aggregated_df_ols['year'] = aggregated_df_ols['date'].dt.year\n",
    "aggregated_df_ols = aggregated_df_ols.drop('date',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model seasonality with dummy vars... do i need this?\n",
    "aggregated_df_ols_dummies =  pd.get_dummies(aggregated_df_ols, columns=['dow','month', 'year'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aggregated_df_ols_dummies.drop('sales',axis=1)\n",
    "# add constant for linear regression\n",
    "X = sm.add_constant(X)\n",
    "X = X.astype(int)\n",
    "y=aggregated_df_ols_dummies['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_OLS = sm.OLS(y,X).fit()\n",
    "model_summary = model_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pvalues' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pvalues2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mpvalues\u001b[49m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      2\u001b[0m pvalues2\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mholiday\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m} , inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m pvalues2\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pvalues' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features that have p value >.10 \n",
    "# ... no xmas day in trainset\n",
    "pvalues = model_OLS.pvalues\n",
    "alpha = .05\n",
    "drop_columns = pvalues[pvalues>alpha]\n",
    "drop_cols = list(drop_columns.index)\n",
    "# manually keeping non holiday columns\n",
    "#most date columns were significant except for these 3 months. still would like to keep so there arent holes in date\n",
    "drop_cols.remove('month_3')\n",
    "drop_cols.remove('month_7')\n",
    "drop_cols.remove('month_9')\n",
    "drop_cols.remove('month_10')\n",
    "drop_cols.remove('month_11')\n",
    "drop_cols.remove('city_Ambato')\n",
    "drop_cols.remove('state_Tungurahua')\n",
    "drop_cols.remove('state_Guayas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values2 = pd.DataFrame(pvalues).reset_index()\n",
    "#pvalues2.rename(columns={'index':'holiday',0:'p'} , inplace=True)\n",
    "#pvalues2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 holidays droped out of 103 . 20 unique holidays remain\n"
     ]
    }
   ],
   "source": [
    "print(len(drop_cols), 'holidays droped out of', len(unique_holis2),'.', len(unique_holis2)-len(drop_cols) ,'unique holidays remain' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holi_shortened = df_holidays_real[~df_holidays_real['description'].isin(drop_cols)]\n",
    "df_holi_shortened = df_holi_shortened.drop(['type','locale','locale_name','transferred'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique holidays from the reduced holidays list\n",
    "unique_holidays = df_holi_shortened['description'].unique()\n",
    "filtered_holidays = df_holidays[df_holidays['description'].isin(unique_holidays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate these out because they have differenct merge conditions\n",
    "national_holidays_filtered = filtered_holidays[filtered_holidays['locale']=='National'].loc[:,('date','description')]\n",
    "local_holidays_filtered = filtered_holidays[filtered_holidays['locale']=='Local'].loc[:,('date','description','locale_name')]\n",
    "state_holidays_filtered = filtered_holidays[filtered_holidays['locale']=='Regional'].loc[:,('date','description','locale_name')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique holidays from the reduced holidays list\n",
    "unique_holidays = df_holi_shortened['description'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine train to holidays ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique holidays from the reduced holidays list\n",
    "filtered_holidays = df_holidays[df_holidays['description'].isin(unique_holidays)]\n",
    "#separate locales out because they have differenct merge conditions\n",
    "national_holidays_filtered = filtered_holidays[filtered_holidays['locale']=='National'].loc[:,('date','description')]\n",
    "local_holidays_filtered = filtered_holidays[filtered_holidays['locale']=='Local'].loc[:,('date','description','locale_name')]\n",
    "state_holidays_filtered = filtered_holidays[filtered_holidays['locale']=='Regional'].loc[:,('date','description','locale_name')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_holiday_merge(train, national_holidays_filtered, state_holidays_filtered, local_holidays_filtered):\n",
    "    train['date'] = pd.to_datetime(train['date'])\n",
    "    df_train_filtered = pd.merge(train, national_holidays_filtered, how='left', on='date')\n",
    "    df_train_filtered = pd.merge(df_train_filtered, state_holidays_filtered, how='left', left_on=['date', 'state'], right_on=['date','locale_name'])\n",
    "    df_train_filtered = pd.merge(df_train_filtered, local_holidays_filtered, how='left', left_on=['date', 'city'], right_on=['date','locale_name'])\n",
    "    df_train_filtered['holiday'] = df_train_filtered['description_x'].combine_first(df_train_filtered['description_y']).combine_first(df_train_filtered['description'])\n",
    "\n",
    "    df_train_filtered = df_train_filtered.drop(['locale_name_x','locale_name_y','description','description_x','description_y'],axis=1)\n",
    "    return df_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_filtered = train_to_holiday_merge(df_train_shortened, national_holidays_filtered, state_holidays_filtered,local_holidays_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine oil and train ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_oil_merge(train, oil):\n",
    "    df_oil['date'] = pd.to_datetime(df_oil['date'])\n",
    "    df = pd.merge(train,oil, how='left', on='date')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>cluster</th>\n",
       "      <th>holiday</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>1091.000</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778826</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>9</td>\n",
       "      <td>POULTRY</td>\n",
       "      <td>438.133</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778827</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>9</td>\n",
       "      <td>PREPARED FOODS</td>\n",
       "      <td>154.553</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778828</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>9</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>2419.729</td>\n",
       "      <td>148</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778829</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>9</td>\n",
       "      <td>SCHOOL AND OFFICE SUPPLIES</td>\n",
       "      <td>121.000</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778830</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>9</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>16.000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2778831 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  store_nbr                      family     sales  \\\n",
       "0       2013-01-02          1                  AUTOMOTIVE     2.000   \n",
       "1       2013-01-02          1                   BABY CARE     0.000   \n",
       "2       2013-01-02          1                      BEAUTY     2.000   \n",
       "3       2013-01-02          1                   BEVERAGES  1091.000   \n",
       "4       2013-01-02          1                       BOOKS     0.000   \n",
       "...            ...        ...                         ...       ...   \n",
       "2778826 2017-08-15          9                     POULTRY   438.133   \n",
       "2778827 2017-08-15          9              PREPARED FOODS   154.553   \n",
       "2778828 2017-08-15          9                     PRODUCE  2419.729   \n",
       "2778829 2017-08-15          9  SCHOOL AND OFFICE SUPPLIES   121.000   \n",
       "2778830 2017-08-15          9                     SEAFOOD    16.000   \n",
       "\n",
       "         onpromotion  cluster holiday  dcoilwtico  \n",
       "0                  0       13     NaN       93.14  \n",
       "1                  0       13     NaN       93.14  \n",
       "2                  0       13     NaN       93.14  \n",
       "3                  0       13     NaN       93.14  \n",
       "4                  0       13     NaN       93.14  \n",
       "...              ...      ...     ...         ...  \n",
       "2778826            0        6     NaN       47.57  \n",
       "2778827            1        6     NaN       47.57  \n",
       "2778828          148        6     NaN       47.57  \n",
       "2778829            8        6     NaN       47.57  \n",
       "2778830            0        6     NaN       47.57  \n",
       "\n",
       "[2778831 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_merged = train_to_oil_merge(df_train_filtered, df_oil)\n",
    "\n",
    "df_train_merged = df_train_merged.drop(['id','city','state', 'type'], axis=1)\n",
    "\n",
    "df_train_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_merged.to_pickle(df_train.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on test data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_transformed = train_to_store_merge(df_test, df_stores)\n",
    "df_test_transformed = train_to_holiday_merge(df_test_transformed, national_holidays_filtered, state_holidays_filtered, local_holidays_filtered)\n",
    "df_test_transformed = train_to_oil_merge(df_test_transformed, df_oil)\n",
    "df_test_transformed = df_test_transformed.drop(['id','city','state', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df_test_transformed, columns=['store_nbr','holiday']) ## add family pca here maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dummies.to_pickle(df_test.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = pd.read_pickle('df_test.pkl')\n",
    "#df_train = pd.read_pickle('df_train.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
